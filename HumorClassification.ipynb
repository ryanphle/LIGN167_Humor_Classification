{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Need to figure out how to parse data to have the review + the label<br>\n",
    "Need to figure out how to build the baseline neural network<br>\n",
    "Need to figure out how "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)\n",
    "with open(\"../review.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        d = json.loads(line)\n",
    "        data[d['funny'] > 0].append((d['review_id'], d['text'], d['funny'] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "sentences = random.sample(data[True], num_samples)\n",
    "sentences.extend(random.sample(data[False], num_samples))\n",
    "random.shuffle(sentences)\n",
    "\n",
    "del data[True]\n",
    "del data[False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the sentences\n",
    "def parseSentenceBasic(sentence):\n",
    "    sent = sentence[1].strip().split()\n",
    "    return [x.lower() for x in sent]\n",
    "\n",
    "def parseSentences(sentences, p_func):\n",
    "    corpus = []\n",
    "    for s in sentences:\n",
    "        corpus.append(p_func(s))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word embedding model based on the current corpus\n",
    "corpus = parseSentences(sentences, parseSentenceBasic)\n",
    "word2Vec = Word2Vec(corpus, size=256, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordEmbedding(word, word2Vec):\n",
    "    try:\n",
    "        return torch.from_numpy(word2Vec.wv[word])\n",
    "    except KeyError:\n",
    "        return torch.from_numpy(word2Vec.wv[\"<UNK>\"])\n",
    "    \n",
    "def getWordEmbedding2(word, word2Vec):\n",
    "    try:\n",
    "        return word2Vec.wv[word]\n",
    "    except KeyError:\n",
    "        return word2Vec.wv[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Network\n",
    "TODO: Figure out what you actually need to do lol<br>\n",
    "So far, I have the network outlined, but we need to figure out how to train the data<br>\n",
    "We also need to figure out how to define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(256, 100)\n",
    "        self.hidden2 = nn.Linear(100, 150)\n",
    "        self.output = nn.Linear(150, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X.to(device)\n",
    "        \n",
    "        q1 = self.hidden1(X)\n",
    "        h1 = F.relu(q1)\n",
    "        q2 = self.hidden2(h1)\n",
    "        h2 = F.relu(q2)\n",
    "        \n",
    "        return self.output(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShallowNetwork(\n",
       "  (hidden1): Linear(in_features=256, out_features=100, bias=True)\n",
       "  (hidden2): Linear(in_features=100, out_features=150, bias=True)\n",
       "  (output): Linear(in_features=150, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shallow_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the corpus into a list of vectors for input\n",
    "def reviewToVecNaive(s):\n",
    "    result = torch.zeros(len(s),256)\n",
    "    for i in range(len(s)):\n",
    "        result[i] = getWordEmbedding(s[i], word2Vec)\n",
    "    return torch.sum(result, 0)\n",
    "\n",
    "def reviewToVecNaive2(s):\n",
    "    result = torch.zeros(256)\n",
    "    for i in range(len(s)):\n",
    "        result = result.add(getWordEmbedding(s[i], word2Vec))\n",
    "    return result\n",
    "\n",
    "def reviewToVecNaive3(s):\n",
    "    result = np.zeros((len(s), 256), dtype=np.float32)\n",
    "    for i in range(len(s)):\n",
    "        result[i] = getWordEmbedding(s[i], word2Vec)\n",
    "    return torch.from_numpy(np.sum(result, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(corpus)\n",
    "corpusVecs1 = [reviewToVecNaive3(s) for s in corpus[:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIdx = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "shallow_net = ShallowNetwork()\n",
    "shallow_net.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def printOptInfo(optimizer):\n",
    "    print(optimizer.__class__.__name__, optimizer.param_groups[0]['lr'])\n",
    "\n",
    "def trainShallowNet(optimizer, epochs, corpusVecs, printResults=True):\n",
    "    if printResults:\n",
    "        printOptInfo(optimizer)\n",
    "    for epoch in range(epochs): \n",
    "        train_loss = []\n",
    "        shallow_net.train()\n",
    "        total_loss = total_correct = 0\n",
    "        total = trainIdx\n",
    "\n",
    "        for i in range(trainIdx):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            corpusVecs[i] = corpusVecs[i].to(device)\n",
    "\n",
    "            output = shallow_net(corpusVecs[i])\n",
    "            output = torch.unsqueeze(output, 0)\n",
    "            output = output.to(device)\n",
    "\n",
    "            _, x = output.data.topk(1)\n",
    "            target = torch.tensor([int(sentences[i][2])])\n",
    "            target = target.to(device)\n",
    "\n",
    "            if x[0][0] == target:\n",
    "                total_correct += 1\n",
    "\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        if printResults:\n",
    "            print(\"epoch: {}, loss: {}, accuracy: {}\".format(epoch, np.mean(train_loss), total_correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printOptInfo(optimizer):\n",
    "    print(optimizer.__class__.__name__, optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD 0.001\n",
      "epoch: 0, loss: 0.6765921560894698, accuracy: 0.5942\n",
      "epoch: 1, loss: 0.6789048265516758, accuracy: 0.57205\n",
      "SGD 0.001\n",
      "epoch: 0, loss: 0.6757887674849481, accuracy: 0.58495\n",
      "epoch: 1, loss: 0.6723869769826532, accuracy: 0.592\n",
      "epoch: 2, loss: 0.6770411373376847, accuracy: 0.5788\n",
      "epoch: 3, loss: 0.677973353369534, accuracy: 0.57975\n",
      "epoch: 4, loss: 0.6783318033911288, accuracy: 0.5763\n",
      "epoch: 5, loss: 0.680336127974838, accuracy: 0.57725\n",
      "epoch: 6, loss: 0.679368626422435, accuracy: 0.58505\n",
      "epoch: 7, loss: 0.6716068382002414, accuracy: 0.6017\n"
     ]
    }
   ],
   "source": [
    "# Testing results for SDG optimizer with constant lr, diff epochs\n",
    "shallow_net = ShallowNetwork().cuda()\n",
    "o1 = optim.SGD(shallow_net.parameters(), lr=0.001)\n",
    "trainShallowNet(o1, 2, corpusVecs1)\n",
    "\n",
    "shallow_net = ShallowNetwork().cuda()\n",
    "o2 = optim.SGD(shallow_net.parameters(), lr=0.001)\n",
    "trainShallowNet(o2, 8, corpusVecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD 0.0001\n",
      "epoch: 0, loss: 0.697416302045621, accuracy: 0.60095\n",
      "epoch: 1, loss: 0.6538178123567253, accuracy: 0.62615\n",
      "SGD 0.01\n",
      "epoch: 0, loss: nan, accuracy: 0.0195\n",
      "epoch: 1, loss: nan, accuracy: 0.0\n",
      "SGD 0.1\n",
      "epoch: 0, loss: nan, accuracy: 0.00335\n",
      "epoch: 1, loss: nan, accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Testing results for SDG optimizer with constant epochs, diff lr\n",
    "shallow_net = ShallowNetwork().cuda()\n",
    "o3 = optim.SGD(shallow_net.parameters(), lr=0.0001)\n",
    "trainShallowNet(o3, 2, corpusVecs1)\n",
    "\n",
    "shallow_net = ShallowNetwork().cuda()\n",
    "o4 = optim.SGD(shallow_net.parameters(), lr=0.01)\n",
    "trainShallowNet(o4, 2, corpusVecs1)\n",
    "\n",
    "shallow_net = ShallowNetwork().cuda()\n",
    "o5 = optim.SGD(shallow_net.parameters(), lr=0.1)\n",
    "trainShallowNet(o5, 2, corpusVecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD 0.0001\n",
      "epoch: 0, loss: 0.7003838335053064, accuracy: 0.5994\n",
      "epoch: 1, loss: 0.6560078274384141, accuracy: 0.62375\n",
      "epoch: 2, loss: 0.6496948119603098, accuracy: 0.6324\n",
      "epoch: 3, loss: 0.6455893912747502, accuracy: 0.6383\n",
      "epoch: 4, loss: 0.6431354025945067, accuracy: 0.6404\n",
      "epoch: 5, loss: 0.6400601583058014, accuracy: 0.64185\n",
      "epoch: 6, loss: 0.6372200701104477, accuracy: 0.64595\n",
      "epoch: 7, loss: 0.6354311815664172, accuracy: 0.6462\n",
      "epoch: 8, loss: 0.633373790150322, accuracy: 0.64905\n",
      "epoch: 9, loss: 0.6313983280704357, accuracy: 0.6485\n"
     ]
    }
   ],
   "source": [
    "shallow_net = ShallowNetwork().cuda()\n",
    "o_best_sgd = optim.SGD(shallow_net.parameters(), lr=0.0001)\n",
    "trainShallowNet(o_best_sgd, 10, corpusVecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam 0.1\n",
      "epoch: 0, loss: 606.7561558526859, accuracy: 0.49975\n",
      "epoch: 1, loss: 4.72142732155174, accuracy: 0.4976\n",
      "Adam 0.01\n",
      "epoch: 0, loss: 3.656859737342596, accuracy: 0.499\n",
      "epoch: 1, loss: 0.7507467496782542, accuracy: 0.4952\n"
     ]
    }
   ],
   "source": [
    "# Testing results with Adam with 2 epochs\n",
    "shallow_net = ShallowNetwork().cuda()\n",
    "o6 = optim.Adam(shallow_net.parameters(), lr=0.1, betas=(0.9, 0.99)) # Starting with the best results found in the paper\n",
    "trainShallowNet(o6, 2, corpusVecs1)\n",
    "\n",
    "shallow_net = ShallowNetwork().cuda()\n",
    "o7 = optim.Adam(shallow_net.parameters(), lr=0.01) # Starting with the best results found in the paper\n",
    "trainShallowNet(o7, 2, corpusVecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam 0.1\n",
      "epoch: 0, loss: 325.3814041033566, accuracy: 0.4988\n",
      "epoch: 1, loss: 0.7207487521916628, accuracy: 0.49715\n",
      "epoch: 2, loss: 0.7207487520843744, accuracy: 0.49715\n",
      "epoch: 3, loss: 0.7207487520709633, accuracy: 0.49715\n",
      "epoch: 4, loss: 0.7207487522661686, accuracy: 0.49715\n",
      "epoch: 5, loss: 0.7207487523317337, accuracy: 0.49715\n",
      "epoch: 6, loss: 0.7207487522795797, accuracy: 0.49715\n",
      "epoch: 7, loss: 0.7207487520948053, accuracy: 0.49715\n"
     ]
    }
   ],
   "source": [
    "# Testing results with Adam with more epochs using best parameters from before\n",
    "shallow_net = ShallowNetwork().cuda()\n",
    "o_best_adam = optim.Adam(shallow_net.parameters(), lr=0.1, betas=(0.9, 0.99)) # Starting with the best results found in the paper\n",
    "trainShallowNet(o_best_adam, 8, corpusVecs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Run this overnight b/c it's a lot of epochs lmao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_net = ShallowNetwork().cuda()\n",
    "o_best_sgd = optim.SGD(shallow_net.parameters(), lr=0.0001)\n",
    "trainShallowNet(o_best_sgd, 50, corpusVecs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After playing around with the training set, apply results to large test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "testIdx = len(corpusVecs1)\n",
    "\n",
    "def testShallowNet(optimizer, epochs, corpusVecs, printResults=True):\n",
    "    shallow_net.eval()\n",
    "    if printResults:\n",
    "        printOptInfo(optimizer)\n",
    "        \n",
    "    valid_loss = []\n",
    "    vtc = 0\n",
    "    total = testIdx - trainIdx\n",
    "\n",
    "    for i in range(trainIdx, testIdx):\n",
    "        corpusVecs[i] = corpusVecs[i].to(device)\n",
    "\n",
    "        output = shallow_net(corpusVecs1[i])\n",
    "        output = torch.unsqueeze(output,0)\n",
    "        output = output.to(device)\n",
    "\n",
    "        _, x = output.data.topk(1)\n",
    "        target = torch.tensor([int(sentences[i][2])])\n",
    "        target = target.to(device)\n",
    "\n",
    "        if x[0][0] == target:\n",
    "            vtc += 1\n",
    "\n",
    "        loss = loss_function(output, target)\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "    if printResults:\n",
    "        print (\"Valid Loss: \", np.mean(valid_loss), \" Valid Accuracy: \", vtc/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Run this overnight too to get the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_net = ShallowNetwork().cuda()\n",
    "best_optim = 0 # REPLACE\n",
    "best_epochs = 0 # REPLACE\n",
    "testShallowNet(best_optim, best_epochs, corpusVecs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build other datasets with diff embeddings using best optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Punctuation ###\n",
    "import string\n",
    "def parseSentenceRemovePunc(sentence):\n",
    "    sent = sentence[1]\n",
    "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "    sent = sent.strip().split()\n",
    "    return [x.lower() for x in sent]\n",
    "\n",
    "def reviewToVecRemovePunc(s):\n",
    "    result = np.zeros((len(s), 256), dtype=np.float32)\n",
    "    for i in range(len(s)):\n",
    "        result[i] = getWordEmbedding(s[i], word2VecRP)\n",
    "    return torch.from_numpy(np.sum(result, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im', 'so', 'pumped', 'for', 'the', 'runn', 'lol', 'yeet', 'hi', 'there']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parseSentenceRemovePunc((1,\"I'm so pumped!!! for the runn* lol yeet., hi, there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusRP = parseSentences(sentences, parseSentenceRemovePunc)\n",
    "word2VecRP = Word2Vec(corpus, size=256, min_count=1)\n",
    "corpusVecsRP = [reviewToVecRemovePunc(s) for s in corpusRP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
